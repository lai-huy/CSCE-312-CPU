	irmovq $100,%rsp
main:
	pushq	%rbp
	rrmovq	%rsp, %rbp
	irmovq  $48, %r11
	subq	%r11, %rsp
	rmmovq	%r10, -36(%rbp)
	rmmovq	%rsi, -48(%rbp)
	irmovq  $1, %r11
	rmmovq	%r11, -16(%rbp)
	irmovq  $1, %r11
	rmmovq	%r11, -12(%rbp)
	irmovq  $2, %r11
	rmmovq	%r11, -8(%rbp)
	irmovq  $2, %r11
	rmmovq	%r11, -4(%rbp)
	irmovq  $1, %r11
	rmmovq	%r11, -32(%rbp)
	irmovq  $1, %r11
	rmmovq	%r11, -28(%rbp)
	irmovq  $1, %r11
	rmmovq	%r11, -24(%rbp)
	irmovq  $2, %r11
	rmmovq	%r11, -20(%rbp)
	mrmovq	-32(%rbp), %rdx
	mrmovq	-16(%rbp), %rax
	rrmovq	%rdx, %rsi
	rrmovq	%rax, %rdi
	irmovq	$0, %r8
	call	matrixmult
	irmovq	$0, %r8
	ret
	
matrixmult:
	pushq	%rbp
	rrmovq	%rsp, %rbp
	rmmovq	%rdi, -24(%rbp)
	rmmovq	%rsi, -32(%rbp)
	mrmovq	-24(%rbp), %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r9
	mrmovq	-32(%rbp), %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-24(%rbp), %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-32(%rbp), %rax
	irmovq  $8, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r8
	addq	%r9, %r8
	rmmovq	%r8, -16(%rbp)
	mrmovq	-24(%rbp), %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r8
	mrmovq	-32(%rbp), %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-24(%rbp), %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-32(%rbp), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r9, %r8
	rmmovq	%r8, -12(%rbp)
	mrmovq	-24(%rbp), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r9
	mrmovq	-32(%rbp), %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-24(%rbp), %rax
	addq	%r8, %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-32(%rbp), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r8
	addq	%r9, %r8
	rmmovq	%r8, -8(%rbp)
	mrmovq	-24(%rbp), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	mrmovq	(%rax), %r9
	mrmovq	-32(%rbp), %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-24(%rbp), %rax
	irmovq  $8, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r8, %r9
	mrmovq	-32(%rbp), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %rax
	irmovq  $4, %r11
	addq	%r11, %rax
	mrmovq	(%rax), %r8
	addq	%r9, %r8
	rmmovq	%r8, -4(%rbp)
	irmovq	$0, %r8
	popq	%rbp
	ret
